{"name":"ardrone_autonomy","tagline":"ROS Driver for AR-Drone 1.0 & 2.0 based on Parrot's Official SDK","body":"# ardrone_autonomy : A ROS Driver for ARDrone 1.0 & 2.0\r\n\r\n\"ardrone_autonomy\" is a [ROS](http://ros.org/ \"Robot Operating System\") driver for [Parrot AR-Drone](http://http://ardrone.parrot.com/parrot-ar-drone/select-site) quadrocopter. This driver is based on official [AR-Drone SDK](https://projects.ardrone.org/) version 2.0 and supports both AR-Drone 1.0 and 2.0. \"ardrone_autonomy\" is a fork of [AR-Drone Brown](http://code.google.com/p/brown-ros-pkg/wiki/ardrone_brown) driver. This package has been developed in [Autonomy Lab](http://autonomy.cs.sfu.ca) of [Simon Fraser University](http://www.sfu.ca) by [Mani Monajjemi](http://sfu.ca/~mmonajje) ( +other [contributors](#contributors)).\r\n\r\n\r\n## Table of Contents\r\n\r\n- [Updates](#updates)\r\n- [Installation](#installation)\r\n\t- [Pre-requirements](#pre-requirements)\r\n\t- [Installation Steps](#installation-steps)\r\n- [How to Run](#how-to-run)\r\n- [Reading from AR-Drone](#reading-from-ar-drone)\r\n\t- [Update Frequencies ](#update-frequencies-new)\r\n\t- [Legacy Navigation Data](#legacy-navigation-data)\r\n\t- [IMU data](#imu-data)\r\n\t- [Magnetometer Data](#magnetometer-data)\r\n\t- [Selective Navdata (Advanced) ](#selective-navdata-advanced-new)\r\n\t- [Cameras](#cameras)\r\n\t- [Tag Detection](#tag-detection)\r\n\t- [Update Frequencies](#update-frequencies)\r\n- [Sending Commands to AR-Drone](#sending-commands-to-ar-drone)\r\n\t- [Hover Modes ](#hover-modes-new) :new:\r\n- [Coordinate Frames](#coordinate-frames)\r\n- [Services](#services)\r\n\t- [Toggle AR-Drone's Camera](#toggle-ar-drones-camera)\r\n\t- [LED Animations](#led-animations)\r\n\t- [Flight Animations ](#flight-animations-new)\r\n\t- [IMU Calibration](#imu-calibration)\r\n\t- [Flat Trim](#flat-trim)\r\n    - [Record to USB Stick](#record-to-usb-stick) :new:\r\n- [Parameters](#parameters)\r\n\t- [AR-Drone Specific Parameters](#ar-drone-specific-parameters)\r\n\t- [Other Parameters](#other-parameters)\r\n- [License](#license)\r\n- [Contributors](#contributors)\r\n- [FAQ](#faq) :new:\r\n\r\n\r\n\r\n## Updates\r\n\r\n- *October 22 2013*: Update to Parrot SDK 2.0.1 (Fixes crashes on 2.4.x firmwares, no support for flight recorder (yet). **Please check the FAQ section for instructions on how to re-compile the SDK**. (Tested on 2.3.3 and 2.4.x firmwares) \r\n- *February 13 2013*: Support for USB key recording ([More info](https://github.com/AutonomyLab/ardrone_autonomy/pull/53)). Motor PWM added to legacy Navdata.\r\n- *January 9 2013*: ROS Groovy support. Support for zero-command without hovering ([More info](https://github.com/AutonomyLab/ardrone_autonomy/pull/34)). Full configurable Navdata support ([More info](https://github.com/AutonomyLab/ardrone_autonomy/pull/31)). Support for \"Flight Animations\". Support for Real-time navdata and video publishing ([More info](https://github.com/AutonomyLab/ardrone_autonomy/pull/44)). Support for configurable data publishing rate.\r\n- *November 9 2012*: Critical Bug in sending configurations to drone fixed and more parameters are supported ([More info](https://github.com/AutonomyLab/ardrone_autonomy/issues/24)). Separate topic for magnetometer data added ([More info](https://github.com/AutonomyLab/ardrone_autonomy/pull/25)).\r\n- *September 5 2012*: Experimental automatic IMU bias removal.\r\n- *August 27 2012*: Thread-safe SDK data access. Synchronized `navdata` and `camera` topics.\r\n- *August 20 2012*: The driver is now provides ROS standard camera interface.\r\n- *August 17 2012*: Experimental `tf` support added. New published topic `imu`.\r\n- *August 1 2012*: Enhanced `Navdata` message. `Navdata` now includes magnetometer data, barometer data, temperature and wind information for AR-Drone 2. [Issue #2](https://github.com/AutonomyLab/ardrone_autonomy/pull/2)\r\n- *July 27 2012*: LED Animation Support added to the driver as a service\r\n- *July 19 2012*: Initial Public Release\r\n\r\n## Installation\r\n\r\n### Pre-requirements\r\n\r\nThis driver has been tested on Linux machines running Ubuntu 11.10, 12.04 & 12.10 (32 bit and 64 bit). However it should also work on any other mainstream Linux distribution. The driver has been tested on both ROS \"electric\" and \"fuerte\". The AR-Drone SDK has its own build system which usually handles system wide dependencies itself. The ROS package depends on these standard ROS packages: `roscpp`, `image_transport`, `sensor_msgs`, `tf`, `camera_info_manager` and `std_srvs`.\r\n\r\n### Installation Steps\r\n\r\nThe installation follows the same steps needed usually to compile a ROS driver.\r\n\r\n* Get the code: Clone (or download and unpack) the driver to your personal ROS stacks folder (e.g. ~/ros/stacks) and `cd` to it. Please make sure that this folder is in your `ROS_PACKAGE_PATH` environmental variable.\r\n\r\n        ```bash\r\n        $ cd ~/ros/stacks\r\n        $ git clone https://github.com/AutonomyLab/ardrone_autonomy.git\r\n        $ rosstack profile && rospack profile\r\n        $ roscd ardrone_autonomy\r\n        ```\r\n\r\n**NOTE (For advanced users):** Instead of the `master` branch you can use the `dev-unstable` branch for the latest _unstable_ code which may contain bug fixes or new features. This is the branch that all developments happen on. Please use this branch to submit pull requests.\r\n \r\n* Compile the AR-Drone SDK: The driver contains a slightly patched version of AR-Drone 2.0 SDK which is located in `ARDroneLib` directory. To compile it, execute the `./build_sdk.sh`. Any system-wide dependency will be managed by the SDK's build script. You may be asked to install some packages during the installation procedure (e.g `daemontools`). You can verify the success of the SDK's build by checking the `lib` folder.\r\n\r\n        ```bash\r\n        $ ./build_sdk.sh\r\n        $ [After a couple of minutes]\r\n        $ ls ./lib\r\n\r\n        libavcodec.a   libavformat.a    libpc_ardrone_notool.a  libvlib.a\r\n        libavdevice.a  libavutil.a      libsdk.a\r\n        libavfilter.a  libpc_ardrone.a  libswscale.a\r\n        ```\r\n\r\n* Compile the driver: You can easily compile the driver by using `rosmake ardrone_autonomy` command.\r\n\r\n## How to Run\r\n\r\nThe driver's executable node is `ardrone_driver`. You can either use `rosrun ardrone_autonomy ardrone_driver` or put it in a custom launch file with your desired parameters.\r\n\r\n## Reading from AR-Drone\r\n\r\n### Update Frequencies :new:\r\n\r\n**Drone Update Frequencies**: The drone's data transmission update frequency depends on `navdata_demo` parameter. When it is 1, the transmission frequency will be 15Hz, otherwise it will be 200Hz. (`navdata_demo` is a numeric parameter not Boolean, so use 1 and 0 (not True/False) to set/unset it)\r\n\r\n**Driver Update Frequencies**: The driver can operate in two modes: real-time or fixed rate. When the `realtime_navdata` parameter is set to True, the driver will publish the received information instantly. However when it is set to False, the driver will cache the most recent received data, then it will publish that at a fixed rate, configured by `looprate` parameter. The default configuration is: `realtime_navdata=False` and `looprate=50`. \r\n\r\nPlease note that if the `looprate` is smaller than the drone's transmission frequency, there will be data loss. The driver's start-up output shows the current configuration. You can also use `rostopic hz` command to check the publish rate of the driver.\r\n\r\n<pre>\r\n# Default Setting - 50Hz non-realtime update, the drone transmission rate is 200Hz\r\nrosrun ardrone_autonomy ardrone_driver _realtime_navdata:=False  _navdata_demo:=0\r\n\r\n# 200Hz real-time update\r\nrosrun ardrone_autonomy ardrone_driver _realtime_navdata:=True _navdata_demo:=0\r\n\r\n# 15Hz real-rime update\r\nrosrun ardrone_autonomy ardrone_driver _realtime_navdata:=True _navdata_demo:=1\r\n</pre>\r\n\r\n### Legacy Navigation Data\r\n\r\nInformation received from the drone will be published to the `ardrone/navdata` topic. The message type is `ardrone_autonomy::Navdata` and contains the following information:\r\n\r\n* `header`: ROS message header\r\n* `batteryPercent`: The remaining charge of the drone's battery (%)\r\n* `state`: The Drone's current state:\r\n        * 0: Unknown\r\n        * 1: Inited\r\n        * 2: Landed\r\n        * 3,7: Flying\r\n        * 4: Hovering\r\n        * 5: Test (?)\r\n        * 6: Taking off\r\n        * 8: Landing\r\n        * 9: Looping (?)\r\n* `rotX`: Left/right tilt in degrees (rotation about the X axis)\r\n* `rotY`: Forward/backward tilt in degrees (rotation about the Y axis)\r\n* `rotZ`: Orientation in degrees (rotation about the Z axis)\r\n* `magX`, `magY`, `magZ`: Magnetometer readings (AR-Drone 2.0 Only) (TBA: Convention)\r\n* `pressure`: Pressure sensed by Drone's barometer (AR-Drone 2.0 Only) (TBA: Unit)\r\n* `temp` : Temperature sensed by Drone's sensor (AR-Drone 2.0 Only) (TBA: Unit)\r\n* `wind_speed`: Estimated wind speed (AR-Drone 2.0 Only) (TBA: Unit)\r\n* `wind_angle`: Estimated wind angle (AR-Drone 2.0 Only) (TBA: Unit)\r\n* `wind_comp_angle`: Estimated wind angle compensation (AR-Drone 2.0 Only) (TBA: Unit)\r\n* `altd`: Estimated altitude (mm)\r\n* `motor1..4`: Motor PWM values\r\n* `vx`, `vy`, `vz`: Linear velocity (mm/s) [TBA: Convention]\r\n* `ax`, `ay`, `az`: Linear acceleration (g) [TBA: Convention]\r\n* `tm`: Timestamp of the data returned by the Drone returned as number of micro-seconds passed since Drone's boot-up.\r\n\r\n\r\n**NOTE:** The legacy Navdata publishing can be disabled by setting the `enable_legacy_navdata` parameter to `False` (legacy navdata is enabled by default).\r\n\r\n### IMU data\r\n\r\nThe linear acceleration, angular velocity and orientation from the `Navdata` is also published to a standard ROS [`sensor_msgs/Imu`](http://www.ros.org/doc/api/sensor_msgs/html/msg/Imu.html) message. The units are all metric and the reference frame is in `Base` frame. This topic is experimental. The covariance values are specified by specific parameters.\r\n\r\n### Magnetometer Data\r\n\r\nThe normalized magnetometer readings are also published to `ardrone/mag` topic as a standard ROS [`geometry_msgs/Vector3Stamped`](http://www.ros.org/doc/api/geometry_msgs/html/msg/Vector3Stamped.html) message.\r\n\r\n### Selective Navdata (Advanced) :new:\r\n\r\nYou can access almost all sensor readings, debug values and status reports sent from the AR-Drone by using \"Selective Navdata\". If you set any of following parameters to \"True\", their corresponding `Navdata` information will be published to a separate topic. For example if you enable `enable_navdata_time`, the driver will publish AR-Drone time information to `ardrone/navdata_time` topic. Most of the names are self-explanatory. Please consult AR-Drone SDK 2.0's documentation (or source code) for more information. All parameters are set to False by default.\r\n\r\n<pre>\r\nenable_navdata_trims\t        enable_navdata_rc_references \tenable_navdata_pwm\t            enable_navdata_altitude\t\r\nenable_navdata_vision_raw \t    enable_navdata_vision_of\t    enable_navdata_vision\t        enable_navdata_vision_perf\t\r\nenable_navdata_trackers_send\tenable_navdata_vision_detect\tenable_navdata_watchdog\t        enable_navdata_adc_data_frame\t\r\nenable_navdata_video_stream\t    enable_navdata_games\t        enable_navdata_pressure_raw\t    enable_navdata_magneto\t\r\nenable_navdata_wind_speed\t    enable_navdata_kalman_pressure\tenable_navdata_hdvideo_stream\tenable_navdata_wifi\tenable_navdata_zimmu_3000\t\r\n</pre>\r\n\r\n**HINT:** You can `rostopic type ardrone/navdata_time | rosmsg show` command for each topic to inspect its published message's data structure.\r\n\r\n### Cameras\r\n\r\nBoth AR-Drone 1.0 and 2.0 are equipped with two cameras. One frontal camera pointing forward and one vertical camera pointing downward. This driver will create three topics for each drone: `ardrone/image_raw`, `ardrone/front/image_raw` and `ardrone/bottom/image_raw`. Each of these three are standard [ROS camera interface](http://ros.org/wiki/camera_drivers) and publish messages of type [image transport](http://www.ros.org/wiki/image_transport). The driver is also a standard [ROS camera driver](http://www.ros.org/wiki/camera_drivers), therefor if camera calibration information is provided either as a set of ROS parameters or appropriate `ardrone_front.yaml` and/or `ardrone_bottom.yaml`, the information will be published in appropriate `camera_info` topics. Please check the FAQ section for more information.\r\n\r\n* The `ardrone/*` will always contain the selected camera's video stream and information.\r\n\r\n* The way that the other two streams work depend on the type of Drone.\r\n\r\n    * Drone 1\r\n\r\n    Drone 1 supports four modes of video streams: Front camera only, bottom camera only, front camera with bottom camera inside (picture in picture) and bottom camera with front camera inside (picture in picture). According to active configuration mode, the driver decomposes the PIP stream and publishes pure front/bottom streams to corresponding topics. The `camera_info` topic will include the correct image size.\r\n\r\n    * Drone 2\r\n\r\n    Drone 2 does not support PIP feature anymore, therefore only one of `ardrone/front` or `ardrone/bottom` topics will be updated based on which camera is selected at the time.\r\n\r\n### Tag Detection\r\n\r\nThe `Navdata` message also returns the special tags that are detected by the Drone's on-board vision processing system. To learn more about the system and the way it works please consult AR-Drone SDK 2.0's [developers guide](https://projects.ardrone.org/projects/show/ardrone-api/). These tags are being detected on both drone's video cameras on-board at 30fps. To configure (or disable) this feature look at the \"Parameters\" section in this documentation.\r\n\r\nThe detected tags' type and position in Drone's camera frame will be published to the following variables in `Navdata` message:\r\n\r\n* `tags_count`: The number of detected tags.\r\n* `tags_type[]`: Vector of types of detected tags (details below)\r\n* `tags_xc[]`, `tags_yc[]`, `tags_width[]`, `tags_height[]`: Vector of position components and size components for each tag. These numbers are expressed in numbers between [0,1000]. You need to convert them back to pixel unit using the corresponding camera's resolution (can be obtained front `camera_info` topic).\r\n* `tags_orientation[]`: For the tags that support orientation, this is the vector that contains the tag orientation expressed in degrees [0..360).\r\n\r\nBy default, the driver will configure the drone to look for _oriented roundels_ using bottom camera and _2D tags v2_ on indoor shells (_orange-yellow_) using front camera. For information on how to extract information from `tags_type` field. Check the FAQ section in the end.\r\n\r\n### Update Frequencies\r\n\r\nTBA.\r\n\r\n## Sending Commands to AR-Drone\r\n\r\nThe drone will *takeoff*, *land* or *emergency stop/reset* by publishing an `Empty` ROS messages to the following topics: `ardrone/takeoff`, `ardrone/land` and `ardrone/reset` respectively.\r\n\r\nIn order to fly the drone after takeoff, you can publish a message of type [`geometry_msgs::Twist`](http://www.ros.org/doc/api/geometry_msgs/html/msg/Twist.html) to the `cmd_vel` topic.\r\n\r\n        -linear.x: move backward\r\n        +linear.x: move forward\r\n        -linear.y: move right\r\n        +linear.y: move left\r\n        -linear.z: move down\r\n        +linear.z: move up\r\n\r\n        -angular.z: turn left\r\n        +angular.z: turn right\r\n\r\nThe range for each component should be between -1.0 and 1.0. The maximum range can be configured using ROS parameters discussed later in this document. \r\n\r\n### Hover Modes :new:\r\n\r\n`geometry_msgs::Twist` has two other member variable called `angular.x` and `angular.y` which can be used to enable/disable \"auto-hover\" mode. \"auto-hover\" is enabled when all six components are set to **zero**. If you want the drone not to enter \"auto hover\" mode in cases you set the first four components to zero, set `angular.x` and `angular.y` to arbitrary **non-zero** values.\r\n\r\n## Coordinate Frames\r\n\r\nThe driver publishes two [`tf`](http://www.ros.org/wiki/tf) transforms between three reference frames: `${tf_prefix}/${base_prefix}_link`, `${tf_prefix}/${base_prefix}_frontcam` and `${tf_prefix}/${base_prefix}_bottomcam`. The `${tf_prefix}` is ROS standard way to handle multi-robot `tf` trees and can be set using `tf_prefix` parameters, by default it is empty. The `${base_link}` is the shared name prefix of all three reference frames and can also be set using parameters, by default it has the value of `ardrone_base`. Using default parameters, the three frames would be: `ardrone_base_link`, `ardrone_base_frontcam` and `ardrone_base_bottomcam`. By default the root frame is  `ardrone_base_link`. Therefor `ardrone_base_frontcam` and `ardrone_base_bottomcam` are children of `ardrone_base_link` in the published `tf` tree. This can be changed using `root_frame` parameter.\r\n\r\nThe `frame_id` field in header of all published topics (navdata, imu, cameras) will have the appropriate frame names. All frames are [ROS REP 103](http://www.ros.org/reps/rep-0103.html) compatible.\r\n\r\n## Services\r\n\r\n### Toggle AR-Drone's Camera\r\n\r\nCalling `ardrone/togglecam` service with no parameters will change the active video camera stream. (e.g `rosservice call /ardrone/togglecam`).\r\n\r\n`ardrone/setcamchannel` service directly sets the current active camera channel. One parameter (`uint8 channel\r\n`) should be sent to this service. For AR-Drone 1.0 the valid values are [0..3] and for AR-Drone 2.0 the valid values are [0..1]. The order is similar to the order described in \"Cameras\" section.\r\n\r\n### LED Animations\r\n\r\nCalling `ardrone/setledanimation` service will invoke one of 14 pre-defined LED animations for the drone. The parameters are\r\n\r\n* `uint8 type`: The type of animation which is a number in range [0..13]\r\n* `float32 freq`: The frequency of the animation in Hz\r\n* `uint8 duration`: The duration of the animation in Seconds.\r\n\r\nThe `type` parameter will map [in order] to one of these animations (check `srv/LedAnim.srv` for more details):\r\n\r\n        BLINK_GREEN_RED, BLINK_GREEN, BLINK_RED, BLINK_ORANGE,\r\n        SNAKE_GREEN_RED, FIRE, STANDARD, RED, GREEN, RED_SNAKE,BLANK,\r\n        LEFT_GREEN_RIGHT_RED, LEFT_RED_RIGHT_GREEN, BLINK_STANDARD`\r\n\r\nYou can test these animations in command line using commands like `rosservice call /ardrone/setledanimation 1 4 5`\r\n\r\n### Flight Animations :new:\r\n\r\nCalling `ardrone/setflightanimation` service will execute one of 20 pre-defined flight animations for the drone. The parameters are:\r\n\r\n* `uint8 type`: The type of flight animation, a number in range [0..19]\r\n* `uint16 duration`: The duration of the animation. Use 0 for default duration (recommended)\r\n\r\nThe `type` parameter will map [in order] to one of these pre-defined animations (check `srv/FlightAnim.srv` for more details):\r\n\r\n    ARDRONE_ANIM_PHI_M30_DEG, ARDRONE_ANIM_PHI_30_DEG, ARDRONE_ANIM_THETA_M30_DEG, ARDRONE_ANIM_THETA_30_DEG,\r\n    ARDRONE_ANIM_THETA_20DEG_YAW_200DEG, ARDRONE_ANIM_THETA_20DEG_YAW_M200DEG, ARDRONE_ANIM_TURNAROUND,\r\n    ARDRONE_ANIM_TURNAROUND_GODOWN, ARDRONE_ANIM_YAW_SHAKE, ARDRONE_ANIM_YAW_DANCE, ARDRONE_ANIM_PHI_DANCE,\r\n    ARDRONE_ANIM_THETA_DANCE, ARDRONE_ANIM_VZ_DANCE, ARDRONE_ANIM_WAVE, ARDRONE_ANIM_PHI_THETA_MIXED,\r\n    ARDRONE_ANIM_DOUBLE_PHI_THETA_MIXED, ARDRONE_ANIM_FLIP_AHEAD, ARDRONE_ANIM_FLIP_BEHIND, ARDRONE_ANIM_FLIP_LEFT,\r\n    ARDRONE_ANIM_FLIP_RIGHT\r\n\r\nYou can test these animations in command line using commands like `rosservice call /ardrone/setflightanimation 1 0` while drone is flying.\r\n\r\nPlease be extra cautious about using animations, especially flip animations.\r\n\r\n### IMU Calibration\r\n\r\nIf `do_imu_caliberation` parameter is set to true, calling `ardrone/imu_recalib` service will make the driver recalculate the biases in IMU data based on data from a short sampling period.\r\n\r\n### Flat Trim\r\n\r\nCalling `ardrone/flattrim` service without any parameter will send a \"Flat Trim\" request to AR-Drone to re-calibrate its rotation estimates assuming that it is on a flat surface. Do not call this service while Drone is flying or while the drone is not actually on a flat surface.\r\n\r\n### Record to USB Stick\r\n\r\nCalling `ardrone/setrecord` service will enable and disable recording to the USB stick. The service takes a simple `1` to enable or `0` to disable. So you can turn on recording to the USB stick with `rosservice call /ardrone/setrecord 1`\r\n\r\n## Parameters\r\n\r\n### AR-Drone Specific Parameters\r\n\r\nThe parameters listed below are named according to AR-Drone's SDK 2.0 configuration. Unless you set the parameters using `rosparam` or in your `launch` file, the default values will be used. These values are applied during driver's initialization phase. Please refer to AR-Drone SDK 2.0's [developer's guide](https://projects.ardrone.org/projects/show/ardrone-api/) for information about valid values. Not all the parameters will be needed during regular usage of the AR-Drone, please consult the example launch file `launch/ardrone.launch` for frequent parameters.\r\n\r\n    altitude, altitude_max, altitude_min, ardrone_name, autonomous_flight, bitrate, bitrate_ctrl_mode, \r\n    bitrate_storage, codec_fps, com_watchdog, control_iphone_tilt, control_level, control_vz_max, \r\n    control_yaw, detect_type, detections_select_h, detections_select_v, detections_select_v_hsync, \r\n    enemy_colors, enemy_without_shell, euler_angle_max, flight_anim, flight_without_shell, flying_mode, \r\n    groundstripe_colors, hovering_range, indoor_control_vz_max, indoor_control_yaw, indoor_euler_angle_max, \r\n    latitude, leds_anim, longitude, manual_trim, max_bitrate, max_size, navdata_demo, navdata_options, \r\n    nb_files, outdoor, outdoor_control_vz_max, outdoor_control_yaw, outdoor_euler_angle_max, output, \r\n    owner_mac, ssid_multi_player, ssid_single_player, travelling_enable, travelling_mode, ultrasound_freq, \r\n    ultrasound_watchdog, userbox_cmd, video_channel, video_codec, video_enable, video_file_index, \r\n    video_live_socket, video_on_usb, video_slices, vision_enable, wifi_mode, wifi_rate\r\n\r\n[This wiki page](https://github.com/AutonomyLab/ardrone_autonomy/wiki/AR-Drone-Parameters) includes more information about each of above parameters.\r\n \r\n### Other Parameters\r\n\r\nThese parameters control the behaviour of the driver.\r\n\r\n* `drone_frame_id` - The \"frame_id\" prefix to be used in all `tf` frame names - default: \"ardrone_base\"\r\n* `root_frame` - The default root in drone's `tf` tree (0: _link, 1: _frontcam, 2: _bottomcam) - Default: 0\r\n* `cov/imu_la`, `cov/imu_av` & `cov/imu_or`: List of 9 covariance values to be used in `imu`'s topic linear acceleration, angular velocity and orientation fields respectively - Default: 0.0 for all members (Please check the FAQ section for a sample launch file that shows how to set these values)\r\n* `do_imu_calibration`: [EXPERIMENTAL] Should the drone cancel the biases in IMU data - Default: 0\r\n* `enable_legacy_navdata`: Enable legacy `Navdata` publish - Default: True\r\n\r\n## License\r\n\r\nThe Parrot's license, copyright and disclaimer for `ARDroneLib` are included with the package and can be found in `ParrotLicense.txt` and `ParrotCopyrightAndDisclaimer.txt` files respectively. The other parts of the code are subject to `BSD` license.\r\n\r\n## Contributors\r\n\r\n[List of all commiters to the repository](http://autonomylab.org/ardrone_autonomy/contribution.html).\r\n\r\n- [Mike Hamer](https://github.com/mikehamer) - Added support for proper SDK2 way of configuring the Drone via parameter (critical bug fix) ([More Info](https://github.com/AutonomyLab/ardrone_autonomy/pull/26)). Support for zero-command without hovering ([More info](https://github.com/AutonomyLab/ardrone_autonomy/pull/34)). Full configurable Navdata support ([More info](https://github.com/AutonomyLab/ardrone_autonomy/pull/31)). Support for Real-time navdata and video publishing ([More info](https://github.com/AutonomyLab/ardrone_autonomy/pull/44)). Support for configurable data publishing rate.\r\n- [Jacokb Engel](https://github.com/JakobEngel)\r\n- [Sameer Parekh](https://github.com/sameerparekh) - [Turn on and off USB stick recording](https://github.com/AutonomyLab/ardrone_autonomy/pull/53) - [Seperate Magnetometer Topic](https://github.com/AutonomyLab/ardrone_autonomy/pull/25)\r\n- [Devmax](https://github.com/devmax) - [Flat Trim](https://github.com/AutonomyLab/ardrone_autonomy/issues/18) + Various\r\ncomments for enhancements\r\n- [Rachel Brindle](https://github.com/younata) - [Enhanced Navdata for AR-Drone 2.0](https://github.com/AutonomyLab/ardrone_autonomy/pull/2)\r\n\r\n## FAQ\r\n\r\n### Where should I go next? Is there any ROS package or stack that can be used as a tutorial/sample to use ardrone_autonomy?\r\n\r\nAbsolutely. Here are some examples:\r\n\r\n- [falkor_ardrone](https://github.com/FalkorSystems/falkor_ardrone)\r\n\r\n\"falkor_ardrone\" is a ROS package which uses the \"ardrone_autonomy\" package to implement autonomous control functionality on an AR.Drone.\r\n\r\n- [tum_ardrone](http://www.ros.org/wiki/tum_ardrone)\r\n\r\nState Estimation, Autopilot and GUI for ardrone.\r\n\r\n- [arl_ardrone_examples](https://github.com/parcon/arl_ardrone_examples)\r\n\r\nThis ROS stack includes a series of very basic nodes to show users how to develop applications that use the ardrone_autonomy drivers for the AR drone 1.0 and 2.0 quadrotor robot.\r\n\r\n- [AR Drone Tutorials](https://github.com/mikehamer/ardrone_tutorials)\r\n\r\nThis repository contains the source-code for the Up and flying with the AR.Drone and ROS tutorial series, published on [Robohub](http://www.robohub.org).\r\n\r\n\r\n### How can I report a bug, submit patches or ask for a feature?\r\n\r\n`github` offers a nice and convenient issue tracking and social coding platform, it can be used for bug reports and pull/feature request. This is the preferred method. You can also contact the author directly.\r\n\r\nIf you want to submit a pull request, please submit to `dev-unstable` branch.\r\n\r\n### Why the `ARDroneLib` has been patched?\r\n\r\nThe ARDrone 2.0 SDK has been patched to 1) Enable the lib only build 2) Make its command parsing compatible with ROS and 3) To fix its weird `main()` function issue\r\n\r\n### Why the wifi bandwidth usage is too much?\r\n\r\nThe driver has been configured by default to use the maximum bandwidth allowed to ensure the best quality video stream possible (please take a look at default values in parameters section). That is the reason why the picture quality received from Drone 2.0 using this driver is far better than what you usually get using other software. If for any reason you prefer the lower quality* video stream, change `bitrate_ctrl_mode`, `max_bitrate` and `bitrate` parameters to the default values provided by the AR-Drone developer guide.\r\n\r\n(*) Please note that lower quality does not mean lower resolution. By configuring AR-Drone to use bitrate control with limits, the picture gets blurry when there is a movement.\r\n\r\n### What is the default configuration for the front camera video stream?\r\n\r\n_Drone 1_: 320x240@15fps UVLC Codec\r\n_Drone 2_: 640x360@20fps H264 codec with no record stream\r\n\r\n### How can I extract camera information and tag type from `tags_type[]`?\r\n\r\n`tag_type` contains information for both source and type of each detected tag. In order to extract information from them you can use the following c macros and enums (taken from `ardrone_api.h`)\r\n\r\n```c++\r\n#define DETECTION_EXTRACT_SOURCE(type)  ( ((type)>>16) & 0x0FF )\r\n#define DETECTION_EXTRACT_TAG(type)     ( (type) & 0x0FF )\r\n\r\ntypedef enum\r\n{\r\n  DETECTION_SOURCE_CAMERA_HORIZONTAL=0,   /*<! Tag was detected on the front camera picture */\r\n  DETECTION_SOURCE_CAMERA_VERTICAL,       /*<! Tag was detected on the vertical camera picture at full speed */\r\n  DETECTION_SOURCE_CAMERA_VERTICAL_HSYNC, /*<! Tag was detected on the vertical camera picture inside the horizontal pipeline */\r\n  DETECTION_SOURCE_CAMERA_NUM,\r\n} DETECTION_SOURCE_CAMERA;\r\n\r\ntypedef enum\r\n{\r\n  TAG_TYPE_NONE             = 0,\r\n  TAG_TYPE_SHELL_TAG        ,\r\n  TAG_TYPE_ROUNDEL          ,\r\n  TAG_TYPE_ORIENTED_ROUNDEL ,\r\n  TAG_TYPE_STRIPE           ,\r\n  TAG_TYPE_CAP              ,\r\n  TAG_TYPE_SHELL_TAG_V2     ,\r\n  TAG_TYPE_TOWER_SIDE       ,\r\n  TAG_TYPE_BLACK_ROUNDEL    ,\r\n  TAG_TYPE_NUM\r\n} TAG_TYPE;\r\n\r\n```\r\n\r\n### How can I calibrate the ardrone front/bottom camera?\r\n\r\nIt is easy to calibrate both cameras using ROS [Camera Calibration](http://www.ros.org/wiki/camera_calibration) package.\r\n\r\nFirst, run the camera_calibration node with appropriate arguments: (For the bottom camera, replace front with bottom)\r\n\r\n```bash\r\nrosrun camera_calibration cameracalibrator.py --size [SIZE] --square [SQUARESIZE] image:=/ardrone/front/image_raw camera:=/ardrone/front\r\n```\r\n\r\nAfter successful calibration, press the `commit` button in the UI. The driver will receive the data from the camera calibration node, then will save the information by default in `~/.ros/camera_info/ardrone_front.yaml`. From this point on, whenever you run the driver on the same computer this file will be loaded automatically by the driver and its information will be published to appropriate `camera_info` topic. Sample calibration files for AR-Drone 2.0's cameras are provided in `data/camera_info` folder.\r\n\r\n### Can I see a sample ardrone node in a launch file to learn how to set parameters?\r\n\r\nYes, you can check the `launch` folder for sample lanuch file.\r\n\r\n### Can I control multiple drones using a single PC? or can I make my drone connect to a wireless router?\r\n\r\nWith some hacking yes! This [wiki page](https://github.com/AutonomyLab/ardrone_autonomy/wiki/Multiple-AR-Drones) contains some information regarding this issue.\r\n\r\n### How to re-compile parrot SDK?\r\n\r\nIf the `git` updates include a SDK upgrade, you'd better cleanup previous vuild files.\r\n\r\n```bash\r\ncd ARDroneLib/Soft/Build\r\nmake clean\r\nrm -rf targets_versions\r\n```\r\n\r\nBuild the SDK and driver again.\r\n\r\n## TODO\r\n\r\n* Make the `tf` publish optional.\r\n* Add the currently selected camera name to `Navdata`\r\n* [DONE] Add separate topic for drone's debug stream (`navdata_demo`)\r\n* [DONE] Make the `togglecam` service accept parameters\r\n* [DONE] Enrich `Navdata` with magneto meter and baro meter information\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}